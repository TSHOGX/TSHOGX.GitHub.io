# 学习理论

Computational Learning Theory

（ML 7.1-7.4；林轩田MLF；西瓜 12）

是一个“可计算与复杂度理论”和“机器学习”的交叉学科，可以看做是用算法分析的套路来分析机器学习模型和问题。用概率和组合数学的方法在某某限制下证明某某的上下限分别是多少

回答了是否可以从数据中拟合到真实的function；也可以由此发展出新的算法

* statistical learning theory（SLT）
* probably approximately correct（PAC）

我们希望 h has small error over D，但是 $err_D(h)$ 不可测量（distribution未知），但是我们可以找到它的上限。我们可以计算的是训练误差 $err_S(h)$，所以希望用training error描述testing error的upper bound：

* True error（generalization error）：$err_D(h) = \Pr(h(x)\not=c^*(x))$，就是不相等的期望（0,1 loss的情况下）对于整个distribution来说的分类错误率
* Training error（Empirical error）：$err_S(h)=\frac 1 m \sum_i I(h(x_i)\not = c^*(x_i))$ ，对于训练数据来说的分类错误率。

两种情况：

* $c^* \in H$ ：Realizable
* $c^* \not\in H$ ：Agnostic learning（找到h close to c*）



**Consistent Learner**：$h(x)=c^*(x),\quad \forall x\in S.$ 

**Version Space (VS)**：$VS_{H,S}=\{h\in H|h(x)=c^*(x), \forall x\in S.\}$ 

**$\epsilon$-exhausted**：$VS_{H,S}$ 被称为 $\epsilon$-exhauste，如果 $(\forall h\in VS_{H,S})err_D(h)<\epsilon$ 

## PAC learning

**PAC Identify**：若对于误差 $\epsilon>0$，置信度 $\delta<1$，所有真实函数 $c\in C$ 和分布 $D$，存在学习算法 $L$，其输出假设 $h\in H$ 满足 $P(E(h)\le \epsilon)\ge 1-\delta$，则称 $L$ 能从假设空间 $H$ 中 PAC Identify $C$  

**PAC Learnable**：若对于任何从分布 $D$ 中 $iid$ 采样得到的训练样本集合 $m$，$m$ 满足 $m\ge {\rm poly}(1/\epsilon,1/\delta,size(\textbf x),size(c))$ ，都有 $L$ 能PAC Identify $C$，则称 $C$ (对于$H$)是PAC Learnable 的.

## VC dimension

VC dimension 描述了假设空间的复杂度

**Growth function**：增长函数表示假设空间 $H$ 对 $m$ 个示例所能赋予标记的最大可能结果数。$\prod_H(m)$ 为假设空间在数据集大小为m时的增长函数。

**dichotomy**：对于二分类问题来说，$H$ 中的假设对 $D$ 中 $m$ 个示例赋予标记的每种可能结果称为对$D$ 的一种对分

**shattering**：若假设空间 $H$ 能实现数据集 $D$ 上的所有对分，则称 $D$ 能被 $H$ 打散

**VC dimension**：假设空间 $H$ 的VC维是能被 $H$ 打散的最大的数据集大小：$VC(H)=max\{m:\prod_H(m)=2^m\}$。与数据分布无关。

growth function和vc dim紧密相关，可以得到给予VC dim的泛化误差上限

**Rademacher complexity**：另一种刻画假设空间复杂度的方法，与 VC dim不同的是，它在一定程度上考虑了数据分布，所以通常比基于 VC dim 的复杂度更紧一点

**Stability**：我们还可以考虑算法的稳定性，即算法在输入发生变化时，输出的变化程度

