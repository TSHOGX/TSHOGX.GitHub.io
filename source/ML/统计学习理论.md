统计学习是基于数据构建统计模型而对数据预测分析，包括 supervised、unsupervised、semi-supervised、reinforcement 等

对于supervised来说，我们希望找到 input space 到 output space 的映射，所有这样的映射/模型构成的集合就是 hypothesis space，即我们的学习范围

模型可以是决策函数 $Y=f(X)$ 或者概率函数 $P(y|x)$，不论哪种模型，X和Y都具有联合概率分布$P(X,Y)$，这是监督学习关于数据的基本假设，因此可以利用统计的方法学习

- **判别模型** discriminative model：直接学习 $P(Y|X)$ or $f(X)$ 。回归，knn，SVM, 条件随机场，boosting，决策树，感知机等。优点：直接面对预测时，往往正确率更高；直接学习模型，简化学习问题
- **生成模型** generative model：对 $P(x,y)$ 建模，生成每个label的最优概率，$P(Y|X) = \frac{P(X,Y)}{P(X)}$。朴素贝叶斯, HMM等。优点：解释性好，可以还原出联合概率分布；存在隐变量也行；学习时收敛速度更快



# 判别分析

判别分析**（discriminant analysis）**是多元统计分析中较为成熟的一种分类方法，它的核心思想是**“分类与判断”**。

## 距离判别基本理论

假设存在两个总体$G_1$和$G_2$，另有$x$为一个$p$维的样本值，计算得到该样本到两个总体的距离$d(x,G_1)$和$d(x,G_2)$，如果$d(x,G_1)$大于$d(x,G_2)$，则认为样本$x$属于总体$G_2$，反之亦然；若两者相等，则该样本待判。所以，最核心的问题在于距离的计算。

* 欧式距离：在计算多个总体之间的距离时并不考虑方差的影响
* 马氏距离：不受指标量纲及指标间相关性的影响（$d^2_{ij} = (x_i-x_j)^T S^{-1}(x_i-x_j), S^{-1}$ 为总体之间的协方差矩阵）

## 贝叶斯判别基本理论

前提是假定我们已经对所要分析的数据有所了解（比如数据服从什么分别，各个类别的先验概率等）

两类总体的概率密度函数和各自出现的先验概率已知，即可推断某样本$x_0$发生时，两类出现的概率大小，即样本属于令后验概率$P(G_i|x_0)$最大的$G$。

## Fisher判别基本理论

Fisher判别法的基本思想是“投影”，将$K$组$P$维的数据向低维空间投影，使其投影的组与组之间的方差尽可能的大，组内的方差尽可能的小。因此，Fisher判别法的重点就是选择适当的“投影轴”。判别函数为就是那个轴。

非线性判别：在判别分析的实际应用中，对复杂的数据使用线性判别可能无法得到理想的效果。为此，我们需要使用类似于二次判别函数的非线性分类方法，将样本点投影到若干种二次曲面中，实现理想的判别效果。

[参考](https://zhuanlan.zhihu.com/The-Art-of-Data)

## 回归分析

回归分析是一种**预测**性的建模技术，它估计了因变量和自变量之间的关系（影响强度和具体关系）。这种技术通常用于预测分析，时间序列模型以及发现变量之间的**因果**关系。

根据**自变量的个数，因变量的类型以及回归线的形状**的不同，有不同回归方法。

在各自类型之下，需要通过最小化损失函数（误差函数）来确定具体的参数值。

常见的回归方程有：

### 线性回归

Linear Regression：因变量是连续的，自变量可以是连续的也可以是离散的，回归线的性质是线性的。

损失函数 -- 最小二乘法：最小化每个数据点到线的垂直偏差平方和来计算最佳拟合线



### 逻辑回归

Logistic Regression：因变量的类型属于二元变量

损失函数 -- 极大似然估计

需要大的样本量



### 逐步回归

Stepwise Regression：自变量的选择是在一个自动的过程中完成的



### 岭回归

Ridge Regression：通过收缩参数λ（lambda）解决多重共线性问题（自变量高度相关）。岭回归通过给回归估计上增加一个偏差度，来降低标准误差。

除常数项以外，这种回归的假设与最小二乘回归类似；它收缩了相关系数的值，但没有达到零，这表明它没有特征选择功能，这是一个正则化方法，并且使用的是L2正则化。

![img](统计学习理论/v2-8b082ec95854efa3bb66dcbbd9be5c36_1440w.jpg)

在这个公式中，有两个组成部分。第一个是最小二乘项，另一个是β2（β-平方）的λ倍，其中β是相关系数。

### 套索回归

Lasso Regression：也会惩罚回归系数的绝对值大小。此外，它能够减少变化程度并提高线性回归模型的精度。

![img](统计学习理论/v2-c60d9285b983bf247f3dae519e757794_1440w.jpg)

Lasso （Least Absolute Shrinkage and Selection Operator）



### ElasticNet

Lasso和Ridge回归技术的混合体。它使用L1来训练并且L2优先作为正则化矩阵。当有多个相关的特征时，ElasticNet是很有用的。Lasso 会随机挑选他们其中的一个，而ElasticNet则会选择两个。



![img](统计学习理论/v2-246720018f1a105b08f4501950dd362d_1440w.jpg)



Lasso和Ridge之间的实际的优点是，它允许ElasticNet继承循环状态下Ridge的一些稳定性。

[参考](